# -*- coding: utf-8 -*-
"""CampañaMarketing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aNKaPbYlnOS3H_gF-TjUr0v8xei_y5ka
"""

# Conexion a Google Colaborative a drive!
from google.colab import drive
drive.mount('/gdrive')

"""### **1. Carga de Modulos**"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

"""### **2. Lectura Inicial del dataset**"""

data = pd.read_csv("/gdrive/My Drive/Trabajo-URP/BankMarketing.csv",sep=";")

data.shape

data.head()

df_pivot = pd.DataFrame({'types': data.dtypes,
                         'nulls': data.isna().sum(),
                          '% nulls': data.isna().sum() / data.shape[0],
                          'size': data.shape[0],
                          'uniques': data.nunique()})
df_pivot
#No hay datos nulos

data.y.value_counts()

sns.factorplot('y',data=data,kind="count")

"""Falta balanceo del dataset"""

data.columns

columnsNumeric = ['age','duration','campaign','pdays','previous','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed']
columnsString = ['job', 'marital', 'education','default','housing','loan','contact','month','day_of_week','poutcome']

# Descripcion o estadisticas basicas de un set de datos
data[columnsNumeric].describe()

"""No hay valores missing, existen valores atipicos"""

data[columnsNumeric].head()

# Es muy importante analizar las variables cuantitativas :
for x in columnsNumeric:
  Q03 = int(data[x].quantile(0.75))+100
  plt.title(data[x].name)
  plt.hist(data[x], bins= 100,range=(0,Q03))
  plt.show()

# Es muy importante analizar las variables cuantitativas, sobre todo respecto al objetivo:
for x in columnsNumeric:
  # Generamos los subsets de comparación
  x1 = data.loc[data.y =='yes', x]
  x2 = data.loc[data.y =='no', x]
  Q03x1 = int(pd.DataFrame(x1)[x].quantile(0.75))+100
  Q03x2 = int(pd.DataFrame(x2)[x].quantile(0.75))+100

  # Mostramos las densidades
  plt.title(data[x].name)
  plt.hist(x1, bins= 100, color='r', range=(0,Q03x1), label='Y')
  plt.hist(x2, bins= 100, color='g', range=(0,Q03x2), label='N')
  plt.legend()
  plt.show()

# El grafico de cajas es muy importante pues nos muestra , dispersion, forma y atipicos:
for x in columnsNumeric:
  plt.title(data[x].name)
  sns.boxplot(x=data[x], palette="Blues");
  plt.show()

# El grafico de cajas es muy importante pues nos muestra , dispersion, forma y atipicos; siempre respecto al target:
for x in columnsNumeric:
  plt.title(data[x].name)
  sns.boxplot(x=data[x], y = data.y);
  plt.show()

# Personas que se han suscrito en mediana tienen menos numero de empleados
# Personas que se han suscrito en mediana tienen menos ratio de cambio en los ultimos 3 meses
# Personas que se han suscrito en mediana tienen mas indice de confianza
# Personas que se han suscrito en mediana tienen menos indice de precios

"""columnas numericas tienen valores atipicos(outliers),variables asimetricas"""

data[columnsString].describe(include='O')

data[columnsString].dtypes

# Mostramos la frecuencia de variables cateóricas para encontras hallazgos
for x in columnsString:
    print(x)
    print(data.groupby(x).size())
    print("\n")

columnsString

#Columnas categoricas
for x in columnsString:
  plt.title(x)
  data.fillna("--NULL").groupby(x)[x].count().plot(kind = "bar")
  plt.show()

sns.boxplot(x='y', y='duration', data=data)

plt.hist(x='age', data=data, bins = 15)

df_age = pd.DataFrame()
df_age['age_y'] = (data[data['y'] == 'yes'][['y','age']].describe())['age']
df_age['age_n'] = (data[data['y'] == 'no'][['y','age']].describe())['age']

df_age.drop(['count','std', '25%', '50%', '75%']).plot.bar()

sns.heatmap(abs(data[columnsNumeric].corr()), annot=True, fmt='.1f', cmap='Blues')
plt.show()

"""Existen correlaciones entre variables"""

data[columnsString].head()

data_categoricas_01 = data[columnsString]
data_categoricas_02 = data[columnsString]

# Tratamiento de Variables Categoricas
# LabelEncoder
from sklearn.preprocessing import LabelEncoder # 1° Importo la funcion!

for c in data_categoricas_01:
    print(str(c))
    le = LabelEncoder()                        # 2° Defino o instancio la funcion!
    le.fit(data_categoricas_01[str(c)])          # 3° Ajuste o entrenamiento!
    data_categoricas_01[str(c)]=le.transform(data_categoricas_01[str(c)])   # 4° Aplicamos!

data_categoricas_01.head()

le.fit(data['y'])     
data['y']=le.transform(data['y'])

# Tratamiento de Variables Categoricas
# Preprocesamiento con OneHotEncoder
data_categoricas_03 = pd.get_dummies(data_categoricas_02)
data_categoricas_03.head()

data2 = pd.concat([data[columnsNumeric],data_categoricas_01,data.y],axis=1)
data2.head()

data2.dtypes

data2.shape

"""### Ingenieria de variables o feature enginnering"""



"""### Seleccion de variables"""





























! pip install woe

# Commented out IPython magic to ensure Python compatibility.
# Anexos : WOE
import woe
from woe.eval import plot_ks
import pandas.core.algorithms as algos
from pandas import Series
import scipy.stats.stats as stats
import re
import traceback
import string
import os
import matplotlib.pyplot as plt
# %matplotlib inline
from pylab import rcParams
rcParams['figure.figsize'] = 14, 8
import warnings
warnings.filterwarnings('ignore')
max_bin = 20
force_bin = 3


# Creamos las Woes - IV
max_bin = 20
force_bin = 3

def mono_bin(Y, X, n = max_bin):
    df1 = pd.DataFrame({"X": X, "Y": Y})
    justmiss = df1[['X','Y']][df1.X.isnull()]
    notmiss = df1[['X','Y']][df1.X.notnull()]
    r = 0
    while np.abs(r) < 1:
        try:
            d1 = pd.DataFrame({"X": notmiss.X, "Y": notmiss.Y, "Bucket": pd.qcut(notmiss.X, n)})
            d2 = d1.groupby('Bucket', as_index=True)
            r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)
            n = n - 1 
        except Exception as e:
            n = n - 1

    if len(d2) == 1:
        n = force_bin         
        bins = algos.quantile(notmiss.X, np.linspace(0, 1, n))
        if len(np.unique(bins)) == 2:
            bins = np.insert(bins, 0, 1)
            bins[1] = bins[1]-(bins[1]/2)
        d1 = pd.DataFrame({"X": notmiss.X, "Y": notmiss.Y, "Bucket": pd.cut(notmiss.X, np.unique(bins),include_lowest=True)}) 
        d2 = d1.groupby('Bucket', as_index=True)
    
    d3 = pd.DataFrame({},index=[])
    d3["MIN_VALUE"] = d2.min().X
    d3["MAX_VALUE"] = d2.max().X
    d3["COUNT"] = d2.count().Y
    d3["EVENT"] = d2.sum().Y
    d3["NONEVENT"] = d2.count().Y - d2.sum().Y
    d3=d3.reset_index(drop=True)
    
    if len(justmiss.index) > 0:
        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])
        d4["MAX_VALUE"] = np.nan
        d4["COUNT"] = justmiss.count().Y
        d4["EVENT"] = justmiss.sum().Y
        d4["NONEVENT"] = justmiss.count().Y - justmiss.sum().Y
        d3 = d3.append(d4,ignore_index=True)
    
    d3["EVENT_RATE"] = d3.EVENT/d3.COUNT
    d3["NON_EVENT_RATE"] = d3.NONEVENT/d3.COUNT
    d3["DIST_EVENT"] = d3.EVENT/d3.sum().EVENT
    d3["DIST_NON_EVENT"] = d3.NONEVENT/d3.sum().NONEVENT
    d3["WOE"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)
    d3["IV"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)
    d3["VAR_NAME"] = "VAR"
    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]       
    d3 = d3.replace([np.inf, -np.inf], 0)
    d3.IV = d3.IV.sum()
    
    return(d3)


def char_bin(Y, X):
        
    df1 = pd.DataFrame({"X": X, "Y": Y})
    justmiss = df1[['X','Y']][df1.X.isnull()]
    notmiss = df1[['X','Y']][df1.X.notnull()]    
    df2 = notmiss.groupby('X',as_index=True)
    
    d3 = pd.DataFrame({},index=[])
    d3["COUNT"] = df2.count().Y
    d3["MIN_VALUE"] = df2.sum().Y.index
    d3["MAX_VALUE"] = d3["MIN_VALUE"]
    d3["EVENT"] = df2.sum().Y
    d3["NONEVENT"] = df2.count().Y - df2.sum().Y
    
    if len(justmiss.index) > 0:
        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])
        d4["MAX_VALUE"] = np.nan
        d4["COUNT"] = justmiss.count().Y
        d4["EVENT"] = justmiss.sum().Y
        d4["NONEVENT"] = justmiss.count().Y - justmiss.sum().Y
        d3 = d3.append(d4,ignore_index=True)
    
    d3["EVENT_RATE"] = d3.EVENT/d3.COUNT
    d3["NON_EVENT_RATE"] = d3.NONEVENT/d3.COUNT
    d3["DIST_EVENT"] = d3.EVENT/d3.sum().EVENT
    d3["DIST_NON_EVENT"] = d3.NONEVENT/d3.sum().NONEVENT
    d3["WOE"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)
    d3["IV"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)
    d3["VAR_NAME"] = "VAR"
    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]      
    d3 = d3.replace([np.inf, -np.inf], 0)
    d3.IV = d3.IV.sum()
    d3 = d3.reset_index(drop=True)
    
    return(d3)


def data_vars(df1, target):
    
    stack = traceback.extract_stack()
    filename, lineno, function_name, code = stack[-2]
    vars_name = re.compile(r'\((.*?)\).*$').search(code).groups()[0]
    final = (re.findall(r"[\w']+", vars_name))[-1]
    
    x = df1.dtypes.index
    count = -1
    
    for i in x:
        if i.upper() not in (final.upper()):
            if np.issubdtype(df1[i], np.number) and len(Series.unique(df1[i])) > 2:
                conv = mono_bin(target, df1[i])
                conv["VAR_NAME"] = i
                count = count + 1
            else:
                conv = char_bin(target, df1[i])
                conv["VAR_NAME"] = i            
                count = count + 1
                
            if count == 0:
                iv_df = conv
            else:
                iv_df = iv_df.append(conv,ignore_index=True)
    
    iv = pd.DataFrame({'IV':iv_df.groupby('VAR_NAME').IV.max()})
    iv = iv.reset_index()
    return(iv_df,iv)

import numpy as np
import pandas as pd
import graphviz, IPython
import matplotlib.pyplot as plt
import matplotlib.lines as lines
from matplotlib.ticker import FuncFormatter
from sklearn.tree import export_graphviz

def draw_tree(tree, df):
    s = export_graphviz(tree, out_file=None, feature_names=df.columns, filled=True)
    return graphviz.Source(s)

def set_rf_samples(n):
    """ Changes Scikit learn's random forests to give each tree a random sample of
    n random rows.
    """
    forest._generate_sample_indices = (lambda rs, n_samples:
        forest.check_random_state(rs).randint(0, n_samples, n))

def reset_rf_samples():
    """ Undoes the changes produced by set_rf_samples.
    """
    forest._generate_sample_indices = (lambda rs, n_samples:
        forest.check_random_state(rs).randint(0, n_samples, n_samples))

# Based on https://github.com/chrispaulca/waterfall.git
def waterfallplot(sample, data, Title="", x_lab="", y_lab="",
		 formatting="{:,.1f}", green_color='#29EA38', red_color='#FB3C62', blue_color='#24CAFF',
		 sorted_value=False, threshold=None, other_label='other', net_label='net', 
		 rotation_value=0, size=None):
	'''
	Given two sequences ordered appropriately, generate a standard waterfall chart.
	Optionally modify the title, axis labels, number formatting, bar colors, 
	increment sorting, and thresholding. Thresholding groups lower magnitude changes
	into a combined group to display as a single entity on the chart.
	'''
	
	#convert data and index to np.array
	index = np.array([f'{c}\n({sample[c].iloc[0]})' for c in sample])
	data = np.array(data)
	
	# wip
	#sorted by absolute value 
	if sorted_value: 
		abs_data = abs(data)
		data_order = np.argsort(abs_data)[::-1]
		data = data[data_order]
		index = index[data_order]
	
	#group contributors less than the threshold into 'other' 
	if threshold:
		
		abs_data = abs(data)
		threshold_v = abs_data.max()*threshold
		
		if threshold_v > abs_data.min():
			index = np.append(index[abs_data>=threshold_v],other_label)
			data = np.append(data[abs_data>=threshold_v],sum(data[abs_data<threshold_v]))
	
	changes = {'amount' : data}
	
	#define format formatter
	def money(x, pos):
		'The two args are the value and tick position'
		return formatting.format(x)
	formatter = FuncFormatter(money)
	
	fig, ax = plt.subplots(figsize=size)
	ax.yaxis.set_major_formatter(formatter)

	#Store data and create a blank series to use for the waterfall
	trans = pd.DataFrame(data=changes,index=index)
	blank = trans.amount.cumsum().shift(1).fillna(0)
	
	trans['positive'] = trans['amount'] > 0

	#Get the net total number for the final element in the waterfall
	total = trans.sum().amount
	trans.loc[net_label]= total
	blank.loc[net_label] = total

	#The steps graphically show the levels as well as used for label placement
	step = blank.reset_index(drop=True).repeat(3).shift(-1)
	step[1::3] = np.nan

	#When plotting the last element, we want to show the full bar,
	#Set the blank to 0
	blank.loc[net_label] = 0
	
	#define bar colors for net bar
	trans.loc[trans['positive'] > 1, 'positive'] = 99
	trans.loc[trans['positive'] < 0, 'positive'] = 99
	trans.loc[(trans['positive'] > 0) & (trans['positive'] < 1), 'positive'] = 99
	
	trans['color'] = trans['positive']
	
	trans.loc[trans['positive'] == 1, 'color'] = green_color
	trans.loc[trans['positive'] == 0, 'color'] = red_color
	trans.loc[trans['positive'] == 99, 'color'] = blue_color
	
	my_colors = list(trans.color)
	
	#Plot and label
	my_plot = plt.bar(range(0,len(trans.index)), blank, width=0.5, color='white')
	plt.bar(range(0,len(trans.index)), trans.amount, width=0.6,
			 bottom=blank, color=my_colors)       
								   
	
	# connecting lines - figure out later
	#my_plot = lines.Line2D(step.index, step.values, color = "gray")
	#my_plot = lines.Line2D((3,3), (4,4))
	
	#axis labels
	plt.xlabel("\n" + x_lab)
	plt.ylabel(y_lab + "\n")

	#Get the y-axis position for the labels
	y_height = trans.amount.cumsum().shift(1).fillna(0)
	
	temp = list(trans.amount)
	
	# create dynamic chart range
	for i in range(len(temp)):
		if (i > 0) & (i < (len(temp) - 1)):
			temp[i] = temp[i] + temp[i-1]
	
	trans['temp'] = temp
			
	plot_max = trans['temp'].max()
	plot_min = trans['temp'].min()
	
	#Make sure the plot doesn't accidentally focus only on the changes in the data
	if all(i >= 0 for i in temp):
		plot_min = 0
	if all(i < 0 for i in temp):
		plot_max = 0
	
	if abs(plot_max) >= abs(plot_min):
		maxmax = abs(plot_max)   
	else:
		maxmax = abs(plot_min)
		
	pos_offset = maxmax / 40
	
	plot_offset = maxmax / 15 ## needs to me cumulative sum dynamic

	#Start label loop
	loop = 0
	for index, row in trans.iterrows():
		# For the last item in the list, we don't want to double count
		if row['amount'] == total:
			y = y_height[loop]
		else:
			y = y_height[loop] + row['amount']
		# Determine if we want a neg or pos offset
		if row['amount'] > 0:
			y += (pos_offset*2)
			plt.annotate(formatting.format(row['amount']),(loop,y),ha="center", color = 'g', fontsize=9)
		else:
			y -= (pos_offset*4)
			plt.annotate(formatting.format(row['amount']),(loop,y),ha="center", color = 'r', fontsize=9)
		loop+=1

	#Scale up the y axis so there is room for the labels
	plt.ylim(plot_min-round(3.6*plot_offset, 7),plot_max+round(3.6*plot_offset, 7))
	
	#Rotate the labels
	plt.xticks(range(0,len(trans)), trans.index, rotation=rotation_value)
	
	#add zero line and title
	plt.axhline(0, color='black', linewidth = 0.6, linestyle="dashed")
	plt.title(Title)
	plt.tight_layout()

	return plt

